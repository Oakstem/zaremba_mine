{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "omtN9VZsO6_z",
        "outputId": "e9017d60-920f-4617-f84d-20376c5c2e0e"
      },
      "source": [
        "from argparse import Namespace\n",
        "from args_parser import parse_args\n",
        "from data import Data\n",
        "from data_getter import DataGetter\n",
        "from penndataset import PennDataset\n",
        "from model.model_base import ModelBase\n",
        "from model.model_getter import get_model\n",
        "from model.type import ModelType\n",
        "from training.train import train\n",
        "from training.nll_loss import nll_loss\n",
        "# from torch.nn import NLLLoss\n",
        "import common as cm\n",
        "from board_wrapper import train_w_RunManager\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    args: Namespace = parse_args()\n",
        "\n",
        "    data: Data = DataGetter.get_data(args.batch_size, args.sequence_length)\n",
        "    traindata = PennDataset(data.train_dataset)\n",
        "    testdata = PennDataset(data.test_dataset)\n",
        "    model_gru_no_dropout: ModelBase = get_model(ModelType.GRU, data.vocabulary_size, 0,\n",
        "                                                args.num_of_layers, args.hidden_layer_units,\n",
        "                                                args.weights_uniforming, args.batch_size)\n",
        "    # train_model(\"GRU No Dropout\", model_gru_no_dropout, data, args)\n",
        "    train_w_RunManager(model_gru_no_dropout, traindata, testdata, nll_loss, args, epochs=2)\n",
        "    # model_gru_dropout: ModelBase = get_model(ModelType.GRU, data.vocabulary_size, args.dropout,\n",
        "    #                                          args.num_of_layers, args.hidden_layer_units,\n",
        "    #                                          args.weights_uniforming)\n",
        "    # train_model(\"GRU With Dropout\", model_gru_dropout, data, args)\n",
        "    #\n",
        "    # model_lstm_no_dropout: ModelBase = get_model(ModelType.LSTM, data.vocabulary_size, 0,\n",
        "    #                                              args.num_of_layers, args.hidden_layer_units,\n",
        "    #                                              args.weights_uniforming)\n",
        "    # train_model(\"LSTM No Dropout\", model_lstm_no_dropout, data, args)\n",
        "    #\n",
        "    # model_lstm_dropout: ModelBase = get_model(ModelType.LSTM, data.vocabulary_size, args.dropout,\n",
        "    #                                           args.num_of_layers, args.hidden_layer_units,\n",
        "    #                                           args.weights_uniforming)\n",
        "    # train_model(\"LSTM With Dropout\", model_lstm_dropout, data, args)\n",
        "\n",
        "\n",
        "def train_model(title: str, model: ModelBase, data: Data, args: Namespace):\n",
        "    print(\"Model: \" + title)\n",
        "    print(model)\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    total_epochs_num = args.total_epochs_num\n",
        "    learning_rate = args.learning_rate\n",
        "    first_epoch_modify_lr = args.first_epoch_modify_lr\n",
        "    lr_decrease_factor = args.lr_decrease_factor\n",
        "    device: str or int = args.device\n",
        "\n",
        "    best_model: ModelBase = None\n",
        "    validation_perplexities: [float] = None\n",
        "    test_perplexities: [float] = None\n",
        "    best_model, validation_perplexities, test_perplexities = \\\n",
        "        train(model, data, total_epochs_num, first_epoch_modify_lr, learning_rate, lr_decrease_factor,\n",
        "              args.max_gradients_norm, device)\n",
        "\n",
        "    print(\"Train Summary:\")\n",
        "    print(\"Validation :\" + str(validation_perplexities))\n",
        "    print(\"Test: \" + str(test_perplexities))\n",
        "\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7f13067c1fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0margs_parser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_getter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataGetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpenndataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPennDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/data_getter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}