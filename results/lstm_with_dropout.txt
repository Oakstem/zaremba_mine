Model: LSTM With Dropout
ModelLSTM(
  (embedding): Embedding(10000, 200)
  (rnns): ModuleList(
    (0): LSTM(200, 200)
    (1): LSTM(200, 200)
  )
  (fc): Linear(in_features=200, out_features=10000, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
)
Epoch no = 1 / 39, lr = 1.000, Norm = 4.273, Train loss = 5.898, Train perplexity : 250.102, Test perplexity : 252.618
Epoch no = 2 / 39, lr = 1.000, Norm = 4.109, Train loss = 5.705, Train perplexity : 178.375, Test perplexity : 189.796
Epoch no = 3 / 39, lr = 1.000, Norm = 4.357, Train loss = 5.541, Train perplexity : 147.940, Test perplexity : 163.422
Epoch no = 4 / 39, lr = 1.000, Norm = 4.422, Train loss = 5.441, Train perplexity : 132.442, Test perplexity : 150.200
Epoch no = 5 / 39, lr = 1.000, Norm = 4.559, Train loss = 5.391, Train perplexity : 121.575, Test perplexity : 141.848
Epoch no = 6 / 39, lr = 1.000, Norm = 4.572, Train loss = 5.364, Train perplexity : 114.733, Test perplexity : 136.523
Epoch no = 7 / 39, lr = 0.833, Norm = 4.701, Train loss = 5.278, Train perplexity : 104.581, Test perplexity : 128.739
Epoch no = 8 / 39, lr = 0.694, Norm = 4.737, Train loss = 5.253, Train perplexity : 98.351, Test perplexity : 123.710
Epoch no = 9 / 39, lr = 0.579, Norm = 4.787, Train loss = 5.070, Train perplexity : 92.186, Test perplexity : 119.438
Epoch no = 10 / 39, lr = 0.482, Norm = 4.961, Train loss = 5.109, Train perplexity : 88.778, Test perplexity : 116.484
Epoch no = 11 / 39, lr = 0.402, Norm = 4.933, Train loss = 5.079, Train perplexity : 84.835, Test perplexity : 113.717
Epoch no = 12 / 39, lr = 0.335, Norm = 4.916, Train loss = 5.067, Train perplexity : 82.278, Test perplexity : 111.845
Epoch no = 13 / 39, lr = 0.279, Norm = 4.942, Train loss = 4.970, Train perplexity : 80.173, Test perplexity : 110.142
Epoch no = 14 / 39, lr = 0.233, Norm = 4.916, Train loss = 5.021, Train perplexity : 78.366, Test perplexity : 108.956
Epoch no = 15 / 39, lr = 0.194, Norm = 5.010, Train loss = 4.955, Train perplexity : 76.992, Test perplexity : 107.782
Epoch no = 16 / 39, lr = 0.162, Norm = 4.946, Train loss = 4.964, Train perplexity : 75.530, Test perplexity : 106.942
Epoch no = 17 / 39, lr = 0.135, Norm = 5.069, Train loss = 4.957, Train perplexity : 74.608, Test perplexity : 105.992
Epoch no = 18 / 39, lr = 0.112, Norm = 5.086, Train loss = 4.906, Train perplexity : 73.729, Test perplexity : 105.477
Epoch no = 19 / 39, lr = 0.093, Norm = 5.163, Train loss = 4.990, Train perplexity : 72.983, Test perplexity : 104.757
Epoch no = 20 / 39, lr = 0.078, Norm = 5.219, Train loss = 4.923, Train perplexity : 72.399, Test perplexity : 104.470
Epoch no = 21 / 39, lr = 0.065, Norm = 5.226, Train loss = 4.903, Train perplexity : 71.998, Test perplexity : 104.159
Epoch no = 22 / 39, lr = 0.054, Norm = 5.057, Train loss = 4.910, Train perplexity : 71.535, Test perplexity : 103.673
Epoch no = 23 / 39, lr = 0.045, Norm = 4.984, Train loss = 4.858, Train perplexity : 71.234, Test perplexity : 103.476
Epoch no = 24 / 39, lr = 0.038, Norm = 5.190, Train loss = 4.903, Train perplexity : 70.931, Test perplexity : 103.239
Epoch no = 25 / 39, lr = 0.031, Norm = 5.157, Train loss = 4.894, Train perplexity : 70.648, Test perplexity : 103.084
Epoch no = 26 / 39, lr = 0.026, Norm = 5.309, Train loss = 4.917, Train perplexity : 70.472, Test perplexity : 102.971
Epoch no = 27 / 39, lr = 0.022, Norm = 5.168, Train loss = 4.885, Train perplexity : 70.367, Test perplexity : 102.845
Epoch no = 28 / 39, lr = 0.018, Norm = 5.178, Train loss = 4.870, Train perplexity : 70.206, Test perplexity : 102.767
Epoch no = 29 / 39, lr = 0.015, Norm = 5.366, Train loss = 4.861, Train perplexity : 70.090, Test perplexity : 102.641
Epoch no = 30 / 39, lr = 0.013, Norm = 5.219, Train loss = 4.881, Train perplexity : 70.026, Test perplexity : 102.584
Epoch no = 31 / 39, lr = 0.010, Norm = 5.263, Train loss = 4.882, Train perplexity : 69.973, Test perplexity : 102.524
Epoch no = 32 / 39, lr = 0.009, Norm = 5.175, Train loss = 4.835, Train perplexity : 69.900, Test perplexity : 102.471
Epoch no = 33 / 39, lr = 0.007, Norm = 5.154, Train loss = 4.867, Train perplexity : 69.835, Test perplexity : 102.446
Epoch no = 34 / 39, lr = 0.006, Norm = 5.284, Train loss = 4.845, Train perplexity : 69.793, Test perplexity : 102.401
Epoch no = 35 / 39, lr = 0.005, Norm = 5.164, Train loss = 4.878, Train perplexity : 69.783, Test perplexity : 102.388
Epoch no = 36 / 39, lr = 0.004, Norm = 5.238, Train loss = 4.857, Train perplexity : 69.779, Test perplexity : 102.359
Epoch no = 37 / 39, lr = 0.004, Norm = 5.303, Train loss = 4.892, Train perplexity : 69.761, Test perplexity : 102.347
Epoch no = 38 / 39, lr = 0.003, Norm = 5.281, Train loss = 4.909, Train perplexity : 69.727, Test perplexity : 102.332
Epoch no = 39 / 39, lr = 0.002, Norm = 5.208, Train loss = 4.822 , Train perplexity : 69.717, Test perplexity : 102.320

Train Summary:
Train Perplexities :[   250.10238630540607, 178.37507139344123, 147.9399836750191, 132.4420162519787,
                        121.57498339839307, 114.73336525429265, 104.58102810049597, 98.35114947993883,
                        92.18586870257867, 88.77784939313315, 84.83540285219749, 82.27820120932019,
                        80.17301628662722, 78.36607960181925, 76.99179073032975, 75.53021908415384,
                        74.60836855812845, 73.728851627179, 72.9825019002432, 72.39895883886086,
                        71.9980517252245, 71.53497527825297, 71.23404188800976, 70.93144247390481,
                        70.64806496587597, 70.47230961676247, 70.36661989815481, 70.2062209072621,
                        70.08961421928534, 70.0261075586315, 69.97277937407934, 69.89965956540625,
                        69.83463976475419, 69.7930811777499, 69.78325321920083, 69.77895462083933,
                        69.76075361178488, 69.72693952901066, 69.71691387666925 ]

Test Perplexities :[    252.6181952534901, 189.79566043731427, 163.42165816330802, 150.1998040457465,
                        141.84793503405524, 136.5231846363107, 128.7385608235437, 123.7097894619572,
                        119.43844890630021, 116.4840484597447, 113.71664068779891, 111.84531102675476,
                        110.14158677670989, 108.95608767883178, 107.78216783110469, 106.94152132153383,
                        105.9916069238906, 105.47695925381636, 104.75676149388003, 104.47027931154432,
                        104.15921490050876, 103.6731579602439, 103.47648746526308, 103.23914892661833,
                        103.08434065583332, 102.9714075028737, 102.84492557175868, 102.76716251200787,
                        102.64113292062227, 102.58380301099363, 102.52437678426786, 102.47057227904003,
                        102.4457608510413, 102.40100237022328, 102.38809974488974, 102.35940017295411,
                        102.34721422569484, 102.33191776987044, 102.3195552810265 ]

Process